# -*- coding: utf-8 -*-
"""Cópia de nlp_olist_projeto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHf17ZHwXpzWHB9qqqU1WAODjKv8l0Ny

# Introdução

Esse projeto tem como objetivo utilizar modelos de inteligência artificial (NLP) para automatizar e classificar os reviews de usuários em scores de 1 a 5.

A Olist é uma startup brasileira que atua no segmento de tecnologia para varejo. A empresa fornece soluções que facilitam a gestão de lojas off-line e online (e-commerce). A Olist concentra vendedores que desejam anunciar em marketplaces como Mercado Livre, Americanas e Amazon, de tal forma que os produtos de todos os vendedores ficam em uma loja única visível ao consumidor final. A empresa possui mais de 30 mil lojistas cadastrados, além de 2 milhões de consumidores únicos a cada ano. ([Wikipédia](https://pt.wikipedia.org/wiki/Olist))

Para esse projeto foi utilizado a base de dados da Olist de avaliações e reviews de produtos comercializados, podendo ser encontrada no [Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?select=olist_order_items_dataset.csv).

Para a avaliação de reviews foi escolhido os produtos de Telefonia, no qual resultou em uma amostra com 2050 reviews de produtos de Telefonia.

# Bibliotecas
"""

!pip install --upgrade gensim==4.3.1 scipy==1.12.0 numpy==1.25.2

!python -m spacy download pt_core_news_sm

# Enelvo para normalizacao #https://github.com/thalesbertaglia/enelvo
!pip install enelvo

!pip install torch-summary

# OpenAI para LLM
!pip install openai

# Baixar representação Word2Vec em portugues do nilc cbow 300
!wget http://143.107.183.175:22980/download.php?file=embeddings/wang2vec/cbow_s300.zip -O cbow_s300.zip
!unzip cbow_s300.zip

# Baixar representação Word2Vec em portugues do nilc skip 300
!wget http://143.107.183.175:22980/download.php?file=embeddings/word2vec/skip_s300.zip -O skip_s300.zip
!unzip skip_s300.zip

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import spacy
import torch

from spacy.lang.pt import stop_words
from wordcloud import WordCloud
from enelvo.normaliser import Normaliser
from sklearn.svm import SVC
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split, ParameterGrid
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.decomposition import PCA

from numpy import triu
from gensim.models import KeyedVectors
from tqdm import tqdm
from transformers import BertModel, BertTokenizer
from torch.optim import AdamW
from torchsummary import summary
from openai import OpenAI
from google.colab import drive, userdata as env_colab

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)

drive.mount('/gdrive')

# Pegar a variavel de ambiente do colab que corresponde ao path do projeto no drive
PATH = env_colab.get('PROJECT_PATH')

"""# Info Datasets e Análise Exploratória"""

df_olist_order_items = pd.read_csv(PATH+'/data/olist_order_items_dataset.csv', sep=",")
df_olist_order_reviews = pd.read_csv(PATH+'/data/olist_order_reviews_dataset.csv', sep=",")
df_olist_orders = pd.read_csv(PATH+'/data/olist_orders_dataset.csv', sep=",")
df_olist_products = pd.read_csv(PATH+'/data/olist_products_dataset.csv', sep=",")

df_olist_order_items.head(10)

df_olist_order_reviews.head(10)

df_olist_orders.head(10)

df_olist_products.head(10)

df_olist_order_items_products = df_olist_order_items.merge(df_olist_products, on='product_id')

df_olist_orders_join = df_olist_orders.merge(df_olist_order_items_products, on='order_id')

df_olist_orders_join = df_olist_orders_join.merge(df_olist_order_reviews, on='order_id')

df_olist_orders_join.head()

df_olist_orders_join.iloc[0]

df_olist_orders_join.info()

df_reviews = df_olist_orders_join[['review_id', 'product_category_name', 'review_score', 'review_comment_title', 'review_comment_message']]

df_reviews.info()

categories = df_reviews['product_category_name'].unique()
print("Tamanho : ", len(categories))
print(categories)

"""Escolha de telefonia"""

df_reviews_telefonia = df_reviews.query("product_category_name == 'telefonia'")

df_reviews_telefonia.info()

df_reviews_telefonia_message_not_null = df_reviews_telefonia[df_reviews_telefonia['review_comment_message'].isna() == False]
df_reviews_telefonia_message_not_null = df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_comment_message'].str.strip().str.len() > 0]
df_reviews_telefonia_message_not_null = df_reviews_telefonia_message_not_null.reset_index(drop=True)

df_reviews_telefonia_message_not_null.info()

df_reviews_telefonia_message_not_null.head(10)

df_reviews_telefonia_message_not_null['review_score'].unique()

"""O dataset possui 5 scores"""

df_reviews_telefonia_message_not_null['review_comment_message']

df_reviews_telefonia_message_not_null['review_comment_message'].iloc[10]

df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_score'] == 1]['review_comment_message']

df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_score'] == 2]['review_comment_message']

df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_score'] == 3]['review_comment_message']

df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_score'] == 4]['review_comment_message']

df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_score'] == 5]['review_comment_message']

df_reviews_telefonia_message_not_null.describe()

quantidade_por_score = list()
for score in range(1,6):
    df_tmp_score = df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null['review_score'] == score]['review_comment_message']
    len_tmp_score = len(df_tmp_score)
    quantidade_por_score.append(len_tmp_score)
    print("-"*10)
    print("Score : ", score)
    print(f"Quantidade de Reviews com score {score}: ", len_tmp_score)
    print(f"Percentual de Reviews com score {score}: ", (len_tmp_score/len(df_reviews_telefonia_message_not_null['review_comment_message']))*100, " %")
    print("Exemplo de Reviews : ", df_tmp_score.iloc[3])
    print("-"*10)
    print("\n")
quantidade_por_score = np.array(quantidade_por_score)
print(f"Quantidade total de reviews : {quantidade_por_score.sum()}")

fig = px.bar(
    x=list(range(1, 6)),
    y=quantidade_por_score,
    labels={
        "y": "Quantidade de Reviews",
        "x": "Score"
    },
    title="Quantidade de Reviews por Score"
)
fig.show()

"""# Funções Comuns para plotar gráficos, wordcloud e pareto"""

def plot_wordcloud(df, column_text, column_review_score):
    text_wordcloud_geral = " ".join([text for text in df[column_text]])

    df_score1 = df.query(f"{column_review_score} == 1")
    text_wordcloud_score1 = " ".join([text for text in df_score1[column_text]])

    df_score2 = df.query(f"{column_review_score} == 2")
    text_wordcloud_score2 = " ".join([text for text in df_score2[column_text]])

    df_score3 = df.query(f"{column_review_score} == 3")
    text_wordcloud_score3 = " ".join([text for text in df_score3[column_text]])

    df_score4 = df.query(f"{column_review_score} == 4")
    text_wordcloud_score4 = " ".join([text for text in df_score4[column_text]])

    df_score5 = df.query(f"{column_review_score} == 5")
    text_wordcloud_score5 = " ".join([text for text in df_score5[column_text]])

    wordcloud_geral = WordCloud(max_font_size=200, max_words=1000, background_color="white", width=500, height=500, random_state=42).generate(text_wordcloud_geral)
    wordcloud_score1 = WordCloud(max_font_size=200, max_words=1000, background_color="white", width=500, height=500, random_state=42).generate(text_wordcloud_score1)
    wordcloud_score2 = WordCloud(max_font_size=200, max_words=1000, background_color="white", width=500, height=500, random_state=42).generate(text_wordcloud_score2)
    wordcloud_score3 = WordCloud(max_font_size=200, max_words=1000, background_color="white", width=500, height=500, random_state=42).generate(text_wordcloud_score3)
    wordcloud_score4 = WordCloud(max_font_size=200, max_words=1000, background_color="white", width=500, height=500, random_state=42).generate(text_wordcloud_score4)
    wordcloud_score5 = WordCloud(max_font_size=200, max_words=1000, background_color="white", width=500, height=500, random_state=42).generate(text_wordcloud_score5)

    fig, axs = plt.subplots(2, 3, figsize=(15, 10))

    axs[0, 0].imshow(wordcloud_geral, interpolation="bilinear")
    axs[0, 0].set_title('Geral')
    axs[0, 0].axis("off")

    axs[0, 1].imshow(wordcloud_score1, interpolation="bilinear")
    axs[0, 1].set_title('Score 1')
    axs[0, 1].axis("off")

    axs[0, 2].imshow(wordcloud_score2, interpolation="bilinear")
    axs[0, 2].set_title('Score 2')
    axs[0, 2].axis("off")

    axs[1, 0].imshow(wordcloud_score3, interpolation="bilinear")
    axs[1, 0].set_title('Score 3')
    axs[1, 0].axis("off")

    axs[1, 1].imshow(wordcloud_score4, interpolation="bilinear")
    axs[1, 1].set_title('Score 4')
    axs[1, 1].axis("off")

    axs[1, 2].imshow(wordcloud_score5, interpolation="bilinear")
    axs[1, 2].set_title('Score 5')
    axs[1, 2].axis("off")

    fig.show()

def _mount_df_freq_pareto(df, column, top_k=20):
    dict_count_token = dict()
    count = 0
    for text in df[column]:
        for doc in nlp(text):
            token = str(doc.text).strip()
            number = dict_count_token.get(token)
            if number is None:
                dict_count_token[token] = 1
            else:
                dict_count_token[token] = number + 1
    df_token_count = pd.DataFrame.from_dict(data=dict_count_token, orient="index", columns=["count"]).sort_values("count", ascending=False)
    df_token_count["cum"] = df_token_count["count"].cumsum()
    df_token_count["percent_cum"] = df_token_count["cum"]/df_token_count["count"].sum()
    return df_token_count

def _plot_pareto(df):
    data = [
        go.Bar(
          name = "Quantidade",
          x= df.index,
          y= df["count"],
          marker= {"color": list(np.repeat('rgb(72, 141, 60)', 10)) + list(np.repeat('rgb(72, 87, 60)', len(df.index) - 10))}
        ),
        go.Scatter(
          line= {
            "color": "rgb(192, 57, 43)", # Vermelho
            "width": 3
          },
          name= "Percentual",
          x=  df.index,
          y= df['percent_cum']*100,
          yaxis= "y2",
          mode='lines+markers'
        ),
    ]

    layout = {

      # titulo
      "title": {
        'text': f"Gráfico de Pareto para Tokens"
      },
      "height": 600,

      # Legend
      "legend": {
        "x": 0.79,
        "y": 1.2,
        'orientation': 'h',
      },

      # Yaxis 1

      "yaxis": {
        "title": f"Quantidade",
        "titlefont": {
        "size": 16,
        "color": "rgb(71, 71, 135)",
        "family": "Courier New, monospace"
        },
      },


      # Yaxis 2
      "yaxis2": {
        "side": "right",
        "range": [0, 100],
        "title": f"Percentual",
        "titlefont": {
          "size": 16,
          "color": "rgb(71, 71, 135)",
          "family": "Courier New, monospace"
        },
        "overlaying": "y",
        "ticksuffix": " %",
      },

    }

    fig = go.Figure(data=data, layout=layout)
    fig.show()

def plot_pareto(df, column, top_k=30):
    df_token_count = _mount_df_freq_pareto(df, column)
    _plot_pareto(df_token_count[:top_k])

def plot_pca_score(X, y, title):
    pca = PCA(n_components=2)
    components = pca.fit_transform(X)
    df_components = pd.DataFrame(components)
    df_components["score"] = y
    fig = px.scatter(df_components, x=0, y=1, color='score', title=title)
    fig.show()

def plot_pca_error(X, y_real, y_pred):
    pca = PCA(n_components=2)
    components = pca.fit_transform(X)
    df_components = pd.DataFrame(components)
    df_components["correct"] = (y_real == y_pred)
    fig = px.scatter(df_components, x=0, y=1, color='correct')
    fig.show()

def plot_score_pred_real(X, y_real, y_pred):
    pca = PCA(n_components=2)
    components = pca.fit_transform(X)
    df_components = pd.DataFrame(components)
    df_components["pred"] = y_pred
    df_components["real"] = y_real
    fig = px.scatter(df_components, x=0, y=1, color='pred', facet_col='real')
    fig.show()

def plot_matrix_confusion(y_real, y_pred):
    cm = confusion_matrix(y_real, y_pred)
    fig = px.imshow(cm, text_auto=True, labels=dict(x="Predicted", y="True"), x=list(range(1,6)), y=list(range(1,6)))
    fig.show()

def add_metrics_df(model, accuracy, f1, dataframe):
    df = pd.DataFrame([[model, accuracy, f1]],columns=["model", "accuracy", "f1_score"])
    if dataframe is not None:
        df = pd.concat([dataframe, df]).reset_index(drop=True)
    return df

def plot_loss(list_loss_train, list_loss_eval):
    df_tmp_train = pd.DataFrame({"Epochs": list(range(1, NUM_EPOCH+1)), "Loss": list_loss_train})
    df_tmp_train["type"] = "train"
    df_tmp_val = pd.DataFrame({"Epochs": list(range(1, NUM_EPOCH+1)), "Loss": list_loss_eval})
    df_tmp_val["type"] = "val"
    df_tmp = pd.concat([df_tmp_train, df_tmp_val]).reset_index(drop=True)
    fig = px.line(
        df_tmp,
        x="Epochs",
        y="Loss",
        color="type",
        title="Loss"
    )
    fig.show()


def plot_accuracy(list_accuracy_train, list_accuracy_eval):
    df_tmp_train = pd.DataFrame({"Epochs": list(range(1, NUM_EPOCH+1)), "Accuracy": list_accuracy_train})
    df_tmp_train["type"] = "train"
    df_tmp_val = pd.DataFrame({"Epochs": list(range(1, NUM_EPOCH+1)), "Accuracy": list_accuracy_eval})
    df_tmp_val["type"] = "val"
    df_tmp = pd.concat([df_tmp_train, df_tmp_val]).reset_index(drop=True)
    fig = px.line(
        df_tmp,
        x="Epochs",
        y="Accuracy",
        color="type",
        title="Accuracy"
    )
    fig.show()

"""# Pré-processamento dos dados

Como observamos na análise exploratória, o dataset tem muitos comentários com caracteres especiais, emojis, palavras escritas abreviadas e de forma errada. Assim, iremos utilizar o enelvo a fim de normalizar e corrigir esses dados
"""

norm_enelvo = Normaliser(capitalize_pns=True, capitalize_acs=True, capitalize_inis=True)
sanitize_enelvo = Normaliser(sanitize=True)

def enelvo_normalise(text):
    return norm_enelvo.normalise(text)


def enelvo_sanitize(text):
    return sanitize_enelvo.normalise(text)

df_reviews_telefonia_message_not_null['review_normalise'] = df_reviews_telefonia_message_not_null['review_comment_message'].apply(enelvo_normalise)
df_reviews_telefonia_message_not_null['review_normalise_sanitize'] = df_reviews_telefonia_message_not_null['review_normalise'].apply(enelvo_sanitize)

df_reviews_telefonia_message_not_null

# Carregar Spacy Portugues
nlp = spacy.load("pt_core_news_sm")

# Quantidade de StopWords
len(stop_words.STOP_WORDS)

"não" in stop_words.STOP_WORDS

"""O não é fundamental, pois tem o sentido negativo em scores ruins, logo tiraremos ela das stopwords."""

def clean_word(words):
    list_clean = list()
    for doc in nlp(words):
        if (doc.is_stop and str(doc.text).lower() != "não") or not(doc.is_alpha) or doc.is_digit or doc.is_currency or doc.is_quote or doc.is_punct or doc.like_email or doc.like_num or doc.like_url:
            continue
        else:
            text = doc.text
            #if doc.is_sent_start is True:
            #    text = str(text).capitalize()
            #else:
            text = str(text).lower()
            list_clean.append(text)
    text = " ".join(list_clean)
    if len(str(text).strip()) > 0:
        return text
    else:
        return None

df_reviews_telefonia_message_not_null['review_normalise_clean'] = df_reviews_telefonia_message_not_null['review_normalise_sanitize'].apply(clean_word)

df_reviews_telefonia_message_not_null

df_reviews_telefonia_message_not_null.info()

df_reviews_telefonia_message_not_null = df_reviews_telefonia_message_not_null[df_reviews_telefonia_message_not_null["review_normalise_clean"].isna() == False]

df_reviews_telefonia_message_not_null.info()

"""# EDA - Pareto e WordCloud

## Wordcloud
"""

plot_wordcloud(df_reviews_telefonia_message_not_null, "review_normalise_clean", "review_score")

"""Podemos perceber que em scores mais perto de 5 o não diminui sua quantidade e importância no score. Quando temos o score mais alto percebemos o surgimento de outras palavras de elogio, tais como gostei, recomendo, ótimo, excelente, rápido, parábens, qualidade, dentre outras.

## Pareto

### Geral
"""

plot_pareto(df=df_reviews_telefonia_message_not_null, column="review_normalise_clean", top_k=300)

"""### Score 1"""

plot_pareto(df=df_reviews_telefonia_message_not_null.query("review_score == 1"), column="review_normalise_clean", top_k=25)

"""### Score 2"""

plot_pareto(df=df_reviews_telefonia_message_not_null.query("review_score == 2"), column="review_normalise_clean", top_k=25)

"""### Score 3"""

plot_pareto(df=df_reviews_telefonia_message_not_null.query("review_score == 3"), column="review_normalise_clean", top_k=25)

"""### Score 4"""

plot_pareto(df=df_reviews_telefonia_message_not_null.query("review_score == 4"), column="review_normalise_clean", top_k=25)

"""### Score 5"""

plot_pareto(df=df_reviews_telefonia_message_not_null.query("review_score == 5"), column="review_normalise_clean", top_k=25)

"""# Modelos"""

df_results = pd.DataFrame(columns=["model", "accuracy", "f1_score"])

cbow = KeyedVectors.load_word2vec_format("cbow_s300.txt")
skip = KeyedVectors.load_word2vec_format("skip_s300.txt")
def _embedding(text, model):
    vector = None
    for doc in nlp(text):
        if vector is None:
            try:
                vector = model[doc.text]
            except:
                vector = model["unk"]
        else:
            try:
                vector = vector + model[doc.text]
            except:
                vector = vector + model["unk"]
    return pd.Series(vector)

def embedding_cbow(text):
    return _embedding(text, cbow)

def embedding_skip(text):
    return _embedding(text, skip)

df_preprocessing_model = df_reviews_telefonia_message_not_null[["review_comment_message", "review_normalise", "review_normalise_sanitize" ,"review_normalise_clean", "review_score"]]

# Dividir dataset treino, validacao e teste
X_train, X_val_test, y_train, y_val_test = train_test_split(df_preprocessing_model.drop(columns="review_score"), df_preprocessing_model["review_score"],train_size=0.7, random_state=RANDOM_STATE)
X_train = X_train.reset_index(drop=True)
X_val_test = X_val_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_val_test = y_val_test.reset_index(drop=True)

X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=RANDOM_STATE)
X_val = X_val.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_val = y_val.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)

"""## SVM + BoW"""

# Train
bow = CountVectorizer(strip_accents="unicode")

bow = bow.fit(X_train["review_normalise_clean"])
x_train_preprocessing = bow.transform(X_train["review_normalise_clean"])

svm = SVC(random_state=RANDOM_STATE)
svm = svm.fit(x_train_preprocessing, y_train)
pred_train = svm.predict(x_train_preprocessing)
print(classification_report(y_train, pred_train))

plot_pca_score(x_train_preprocessing, y_train, "Score Base")

# Validacao
x_val_preprocessing = bow.transform(X_val["review_normalise_clean"])
pred_val = svm.predict(x_val_preprocessing)
print(classification_report(y_val, pred_val))

# Teste
x_test_preprocessing = bow.transform(X_test["review_normalise_clean"])
pred_test = svm.predict(x_test_preprocessing)
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("SVM + BoW", accuracy, f1, df_results)
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

plot_pca_error(x_test_preprocessing, y_test, pred_test)

plot_pca_score(x_test_preprocessing, y_test, "Score Real")
plot_pca_score(x_test_preprocessing, pred_test, "Score Pred")

plot_score_pred_real(x_test_preprocessing, y_test, pred_test)

"""## SVM + TFIDF"""

# Train
tfidf = TfidfVectorizer(strip_accents="unicode")

tfidf = tfidf.fit(X_train["review_normalise_clean"])
x_train_preprocessing = tfidf.transform(X_train["review_normalise_clean"])

svm = SVC(random_state=RANDOM_STATE)
svm = svm.fit(x_train_preprocessing, y_train)
pred_train = svm.predict(x_train_preprocessing)
print(classification_report(y_train, pred_train))

plot_pca_score(x_train_preprocessing, y_train, "Score Base")

# Validacao
x_val_preprocessing = tfidf.transform(X_val["review_normalise_clean"])
pred_val = svm.predict(x_val_preprocessing)
print(classification_report(y_val, pred_val))

# Teste
x_test_preprocessing = tfidf.transform(X_test["review_normalise_clean"])
pred_test = svm.predict(x_test_preprocessing)
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("SVM + TFIDF unigrams", accuracy, f1, df_results)
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

"""Podemos notar que o modelo consegue distiguir bem o score máximo (5) e o mínimo (1), mas os demais scores o modelo classifica erroneamente."""

plot_pca_error(x_test_preprocessing, y_test, pred_test)

plot_pca_score(x_test_preprocessing, y_test, "Score Real")
plot_pca_score(x_test_preprocessing, pred_test, "Score Pred")

plot_score_pred_real(x_test_preprocessing, y_test, pred_test)

"""## SVM + TFIDF bigrams"""

# Train
tfidf = TfidfVectorizer(strip_accents="unicode", ngram_range=(1, 2))

tfidf = tfidf.fit(X_train["review_normalise_clean"])
x_train_preprocessing = tfidf.transform(X_train["review_normalise_clean"])

svm = SVC(random_state=RANDOM_STATE)
svm = svm.fit(x_train_preprocessing, y_train)
pred_train = svm.predict(x_train_preprocessing)
print(classification_report(y_train, pred_train))

plot_pca_score(x_train_preprocessing, y_train, "Score Base")

# Validacao
x_val_preprocessing = tfidf.transform(X_val["review_normalise_clean"])
pred_val = svm.predict(x_val_preprocessing)
print(classification_report(y_val, pred_val))

# Teste
x_test_preprocessing = tfidf.transform(X_test["review_normalise_clean"])
pred_test = svm.predict(x_test_preprocessing)
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("SVM + TFIDF bigrams", accuracy, f1, df_results)
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

"""Podemos notar que o modelo classifica bem os scores 1 e 5, mas o score 2 não conseguiu classificar"""

plot_pca_error(x_test_preprocessing, y_test, pred_test)

plot_pca_score(x_test_preprocessing, y_test, "Score Real")
plot_pca_score(x_test_preprocessing, pred_test, "Score Pred")

plot_score_pred_real(x_test_preprocessing, y_test, pred_test)

"""## SVM + Word2Vector CBOW 300"""

# Train
X_train_embedding = X_train["review_normalise_clean"].apply(embedding_cbow)

svm = SVC(random_state=RANDOM_STATE)#, class_weight={1:0.25, 2:0.3, 3:0.25, 4:0.1, 5:0.1})
svm = svm.fit(X_train_embedding, y_train)
pred_train = svm.predict(X_train_embedding)
print(classification_report(y_train, pred_train))

plot_pca_score(X_train_embedding, y_train, "Score Base")

# Validacao
X_val_embedding = X_val["review_normalise_clean"].apply(embedding_cbow)
pred_val = svm.predict(X_val_embedding)
print(classification_report(y_val, pred_val))

# Teste
X_test_embedding = X_test["review_normalise_clean"].apply(embedding_cbow)
pred_test = svm.predict(X_test_embedding)
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("SVM + Word2Vector CBOW 300", accuracy, f1, df_results)
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

plot_pca_error(X_test_embedding, y_test, pred_test)

plot_pca_score(X_test_embedding, y_test, "Score Real")
plot_pca_score(X_test_embedding, pred_test, "Score Pred")

plot_score_pred_real(X_test_embedding, y_test, pred_test)

"""## SVM + Word2Vector SKIP 300"""

# Train
X_train_embedding = X_train["review_normalise_clean"].apply(embedding_skip)

svm = SVC(random_state=RANDOM_STATE)#, class_weight={1:0.25, 2:0.3, 3:0.25, 4:0.1, 5:0.1})
svm = svm.fit(X_train_embedding, y_train)
pred_train = svm.predict(X_train_embedding)
print(classification_report(y_train, pred_train))

plot_pca_score(X_train_embedding, y_train, "Score Base")

# Validacao
X_val_embedding = X_val["review_normalise_clean"].apply(embedding_skip)
pred_val = svm.predict(X_val_embedding)
print(classification_report(y_val, pred_val))

# Teste
X_test_embedding = X_test["review_normalise_clean"].apply(embedding_skip)
pred_test = svm.predict(X_test_embedding)
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("SVM + Word2Vector SKIP 300", accuracy, f1, df_results)
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

plot_pca_error(X_test_embedding, y_test, pred_test)

plot_pca_score(X_test_embedding, y_test, "Score Real")
plot_pca_score(X_test_embedding, pred_test, "Score Pred")

plot_score_pred_real(X_test_embedding, y_test, pred_test)

"""## BERT (Bertimbau Base)"""

def get_best_hyperparameters(num_scores, dropout, max_token, hidden_layer_classification, weight, num_epoch, dict_hiperparameters, X_train, y_train, X_val, y_val):
    best_f1_score = 0
    best_parameter = dict()
    for hiperparameter in tqdm(ParameterGrid(dict_hiperparameters)):
        model_bert = Bert(
            num_scores=num_scores,
            dropout=dropout,
            max_token=max_token,
            hidden_layer_classification=hidden_layer_classification,
            weight=weight
        )
        list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
            X_train=X_train,
            y_train=y_train,
            X_eval=X_val,
            y_eval=y_val,
            learning_rate=hiperparameter["learning_rate"],
            num_epoch=num_epoch,
            batch_size=hiperparameter["batch_size"],
            verbose=False
        )
        pred_test, pred_prob = model_bert.predict(
            X=X_val,
            batch_size=BATCH_SIZE
        )
        f1 = f1_score(y_val, pred_test, average='macro')
        if f1 > best_f1_score:
            best_f1_score = f1
            best_parameter = hiperparameter

    return best_f1_score, best_parameter

"""### Definir quantidade máxima de tokens"""

X_train

tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)
list_num_tokens = list()
for review in X_train["review_normalise"]:
    tokens = tokenizer.encode(
      review,
      max_length=512,
      truncation=True,
      return_attention_mask=True,

    )
    list_num_tokens.append(len(tokens))

# Histograma para definir a Quantidade máxima de tokens
fig = px.histogram(
    x=list_num_tokens,
    labels={
        "x": "max token"
    },
    title="Histograma Quantidade de Tokens"
)
fig.show()

"""### Definição de modelo"""

class ReviewDataset(torch.utils.data.Dataset):

    def __init__(self, reviews, scores, max_token_len):
        self.reviews = reviews
        self.scores = scores
        self.tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)
        self.max_token_len = max_token_len

    def __len__(self):
        return len(self.reviews)

    def __getitem__(self, item):
        review = str(self.reviews[item])
        score = self.scores[item] if self.scores is not None else None

        encoding = tokenizer.encode_plus(
            review,
            add_special_tokens=True,
            max_length=300,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        data = {
            "review": review,
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
        }
        if score is not None:
            data["score"] = torch.tensor(score, dtype=torch.int)#torch.IntTensor([score])

        return data


def create_data_loader(reviews, scores, max_token_len, batch_size):
    ds = ReviewDataset(
        reviews=reviews,
        scores=scores,
        max_token_len=max_token_len
    )

    return torch.utils.data.DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=2
    )

class Bert(torch.nn.Module):
    def __init__(self, num_scores, dropout, max_token, hidden_layer_classification=0, weight=None):
        super().__init__()

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.max_token = max_token

        self.bertimbau = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')

        for param in self.bertimbau.parameters(): # Congelar camadas do bertimbau para treinamento
            param.requires_grad = False

        self.dropout = torch.nn.Dropout(dropout)
        self.batch_normalization = torch.nn.BatchNorm1d(self.bertimbau.config.hidden_size)
        if hidden_layer_classification == 0 or hidden_layer_classification < 0:
            self.head_classification = torch.nn.Linear(self.bertimbau.config.hidden_size, num_scores)
        elif hidden_layer_classification > 0:
            self.head_classification = torch.nn.Sequential(
                torch.nn.Linear(self.bertimbau.config.hidden_size, hidden_layer_classification),
                torch.nn.Linear(hidden_layer_classification, num_scores),
            )

        tensor_weight = None if weight is None else torch.FloatTensor(weight)
        self.loss = torch.nn.CrossEntropyLoss(weight=tensor_weight).to(self.device)

        self.softmax = torch.nn.functional.softmax


    def forward(self, input_ids, attention_mask):
        output_embedding = self.bertimbau(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        output_bertimbau = self.dropout(output_embedding["pooler_output"])
        output_bertimbau = self.batch_normalization(output_bertimbau)
        output = self.head_classification(output_bertimbau)
        return output


    def _train_epoch(
        self,
        dataloader,
        learning_rate,
        verbose=True
    ):
        self.to(self.device)
        self.train()
        optimizer = AdamW(self.parameters(), lr=learning_rate)
        list_loss = list()
        num_correct_pred = 0
        total_pred = 0

        _verb = tqdm if verbose else list

        for data in _verb(dataloader):
            input_ids = data["input_ids"].to(self.device)
            attention_mask = data["attention_mask"].to(self.device)
            score = data["score"].to(self.device)

            outputs = self.forward(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            _, preds = torch.max(outputs, dim=1)

            loss = self.loss(outputs, (score-1).long()) # -1 porque o score vai de 1 a 5 e o output de 0 a 4

            num_correct_pred += torch.sum(preds==(score-1)) # -1 porque o score vai de 1 a 5 e o output de 0 a 4
            total_pred += len(data["input_ids"])

            list_loss.append(loss.item())
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        mean_loss = np.mean(list_loss)
        accuracy = num_correct_pred.int().item() / total_pred

        return accuracy, mean_loss


    def _eval_epoch(
        self,
        dataloader,
        verbose=True
    ):
        self.to(self.device)
        self.eval()

        list_loss = list()
        num_correct_pred = 0
        total_pred = 0

        _verb = tqdm if verbose else list

        with torch.no_grad():
            for data in _verb(dataloader):
                input_ids = data["input_ids"].to(self.device)
                attention_mask = data["attention_mask"].to(self.device)
                score = data["score"].to(self.device)

                outputs = self.forward(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                _, preds = torch.max(outputs, dim=1)

                loss = self.loss(outputs, (score-1).long()) # -1 porque o score vai de 1 a 5 e o output de 0 a 4

                num_correct_pred += torch.sum(preds==(score-1)) # -1 porque o score vai de 1 a 5 e o output de 0 a 4
                total_pred += len(data["input_ids"])

                list_loss.append(loss.item())

        mean_loss = np.mean(list_loss)
        accuracy = num_correct_pred.int().item() / total_pred

        return accuracy, mean_loss


    def fit(
        self,
        X_train,
        y_train,
        X_eval,
        y_eval,
        learning_rate,
        num_epoch,
        batch_size,
        verbose=True
    ):
        ds_train = create_data_loader(X_train, y_train, self.max_token, batch_size)
        ds_eval = create_data_loader(X_eval, y_eval, self.max_token, batch_size)

        list_accuracy_train = list()
        list_accuracy_eval = list()
        list_loss_train = list()
        list_loss_eval = list()

        for epoch in range(num_epoch):
            if verbose:
                print(f"Epoch {epoch+1}")
            # Train
            train_accuracy, train_mean_loss = self._train_epoch(ds_train, learning_rate, verbose=verbose)
            list_accuracy_train.append(train_accuracy)
            list_loss_train.append(train_mean_loss)

            # Eval
            eval_accuracy, eval_mean_loss = self._eval_epoch(ds_eval, verbose=verbose)
            list_accuracy_eval.append(eval_accuracy)
            list_loss_eval.append(eval_mean_loss)

            if verbose:
                print(
                    f"""
                    ---------
                    Epoch {epoch+1}
                    Train : Accuracy = {train_accuracy} Loss = {train_mean_loss}
                    Eval  : Accuracy = {eval_accuracy}  Loss = {eval_mean_loss}
                    ---------
                    """
                )

        self.eval()

        return list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval


    def predict(self, X, batch_size):
        self.to(self.device)
        self.eval()

        list_preds = list()
        list_probs = list()

        dataloader = create_data_loader(reviews=X, scores=None, max_token_len=self.max_token, batch_size=batch_size)

        with torch.no_grad():
            for data in dataloader:
                input_ids = data["input_ids"].to(self.device)
                attention_mask = data["attention_mask"].to(self.device)

                outputs = self.forward(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                probs = self.softmax(outputs, dim=1)
                list_probs.extend(probs)

                _, preds = torch.max(probs, dim=1)
                list_preds.extend(preds+1) # +1 porque o score vai de 1 a 5 e o output de 0 a 4

        list_preds = torch.stack(list_preds).to("cpu").numpy()
        list_probs = torch.stack(list_probs).to("cpu").numpy()

        return list_preds, list_probs

"""### Escolha Arquitetura e Hiperparametros

#### Arquitetura com 1 camada de saida de classificação
"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = None #[0.1, 0.25, 0.25, 0.3, 0.1]
HIDDEN_LAYER_CLASSIFICATION = 0 # quantidades de neuronios na camada oculta antes da saida, caso 0 so vai ter embedding + saida de classificacao

model_bert = Bert(
     num_scores=NUM_SCORES,
     dropout=DROPOUT,
     max_token=MAX_TOKEN,
     hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
     weight=WEIGHT
)

# Train e Val
# recomendacao paper bert
# Batch size: 16, 32
# Learning rate (Adam): 5e-5, 3e-5, 2e-5
# Number of epochs: 2, 3, 4
# recomendacao author bert fine-tuning
#batch sizes: 8, 16, 32, 64, 128
#learning rates: 3e-4, 1e-4, 2e-5, 5e-5, 3e-5
LEARNING_RATE = 1e-4
NUM_EPOCH = 4
BATCH_SIZE = 32

list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_eval=X_val["review_normalise"],
    y_eval=y_val,
    learning_rate=LEARNING_RATE,
    num_epoch=NUM_EPOCH,
    batch_size=BATCH_SIZE
)

plot_loss(list_loss_train, list_loss_eval)

plot_accuracy(list_accuracy_train, list_accuracy_eval)

# Validacao
pred_val, pred_probs = model_bert.predict(X_val["review_normalise"], BATCH_SIZE)
print(classification_report(y_val, pred_val))

plot_matrix_confusion(y_val, pred_val)

"""#### Arquitetura com 2 camada de saida de classificação (1 oculta de 1500 + saida classificacao)"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = None #[0.1, 0.25, 0.25, 0.3, 0.1]
HIDDEN_LAYER_CLASSIFICATION = 1500 # quantidades de neuronios na camada oculta antes da saida, caso 0 so vai ter embedding + saida de classificacao

model_bert = Bert(
     num_scores=NUM_SCORES,
     dropout=DROPOUT,
     max_token=MAX_TOKEN,
     hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
     weight=WEIGHT
)

# Train e Val
# recomendacao paper bert
# Batch size: 16, 32
# Learning rate (Adam): 5e-5, 3e-5, 2e-5
# Number of epochs: 2, 3, 4
# recomendacao author bert fine-tuning
#batch sizes: 8, 16, 32, 64, 128
#learning rates: 3e-4, 1e-4, 2e-5, 5e-5, 3e-5
LEARNING_RATE = 1e-4
NUM_EPOCH = 4
BATCH_SIZE = 32

list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_eval=X_val["review_normalise"],
    y_eval=y_val,
    learning_rate=LEARNING_RATE,
    num_epoch=NUM_EPOCH,
    batch_size=BATCH_SIZE
)

plot_loss(list_loss_train, list_loss_eval)

plot_accuracy(list_accuracy_train, list_accuracy_eval)

# Validacao
pred_val, pred_probs = model_bert.predict(X_val["review_normalise"], BATCH_SIZE)
print(classification_report(y_val, pred_val))

plot_matrix_confusion(y_val, pred_val)

"""#### Arquitetura com 2 camada de saida de classificação (1 oculta de 768 + saida classificacao)"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = None #[0.1, 0.25, 0.25, 0.3, 0.1]
HIDDEN_LAYER_CLASSIFICATION = 768 # quantidades de neuronios na camada oculta antes da saida, caso 0 so vai ter embedding + saida de classificacao

model_bert = Bert(
     num_scores=NUM_SCORES,
     dropout=DROPOUT,
     max_token=MAX_TOKEN,
     hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
     weight=WEIGHT
)

# Train e Val
# recomendacao paper bert
# Batch size: 16, 32
# Learning rate (Adam): 5e-5, 3e-5, 2e-5
# Number of epochs: 2, 3, 4
# recomendacao author bert fine-tuning
#batch sizes: 8, 16, 32, 64, 128
#learning rates: 3e-4, 1e-4, 2e-5, 5e-5, 3e-5
LEARNING_RATE = 1e-4
NUM_EPOCH = 4
BATCH_SIZE = 32

list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_eval=X_val["review_normalise"],
    y_eval=y_val,
    learning_rate=LEARNING_RATE,
    num_epoch=NUM_EPOCH,
    batch_size=BATCH_SIZE
)

plot_loss(list_loss_train, list_loss_eval)

plot_accuracy(list_accuracy_train, list_accuracy_eval)

# Validacao
pred_val, pred_probs = model_bert.predict(X_val["review_normalise"], BATCH_SIZE)
print(classification_report(y_val, pred_val))

plot_matrix_confusion(y_val, pred_val)

"""#### Arquitetura com 2 camada de saida de classificação (1 oculta de 350 + saida classificacao)"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = None #[0.1, 0.25, 0.25, 0.3, 0.1]
HIDDEN_LAYER_CLASSIFICATION = 350 # quantidades de neuronios na camada oculta antes da saida, caso 0 so vai ter embedding + saida de classificacao

model_bert = Bert(
     num_scores=NUM_SCORES,
     dropout=DROPOUT,
     max_token=MAX_TOKEN,
     hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
     weight=WEIGHT
)

# Train e Val
# recomendacao paper bert
# Batch size: 16, 32
# Learning rate (Adam): 5e-5, 3e-5, 2e-5
# Number of epochs: 2, 3, 4
# recomendacao author bert fine-tuning
#batch sizes: 8, 16, 32, 64, 128
#learning rates: 3e-4, 1e-4, 2e-5, 5e-5, 3e-5
LEARNING_RATE = 1e-4
NUM_EPOCH = 4
BATCH_SIZE = 32

list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_eval=X_val["review_normalise"],
    y_eval=y_val,
    learning_rate=LEARNING_RATE,
    num_epoch=NUM_EPOCH,
    batch_size=BATCH_SIZE
)

plot_loss(list_loss_train, list_loss_eval)

plot_accuracy(list_accuracy_train, list_accuracy_eval)

# Validacao
pred_val, pred_probs = model_bert.predict(X_val["review_normalise"], BATCH_SIZE)
print(classification_report(y_val, pred_val))

plot_matrix_confusion(y_val, pred_val)

"""#### Melhor Arquitetura com correção de balanceamento na rede"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = [0.1, 0.25, 0.25, 0.3, 0.1] # Pesos para classe a fim de balancear a rede no treinamento
HIDDEN_LAYER_CLASSIFICATION = 1500 # quantidades de neuronios na camada oculta antes da saida, caso 0 so vai ter embedding + saida de classificacao

model_bert = Bert(
     num_scores=NUM_SCORES,
     dropout=DROPOUT,
     max_token=MAX_TOKEN,
     hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
     weight=WEIGHT
)

# Train e Val
# recomendacao paper bert
# Batch size: 16, 32
# Learning rate (Adam): 5e-5, 3e-5, 2e-5
# Number of epochs: 2, 3, 4
# recomendacao author bert fine-tuning
#batch sizes: 8, 16, 32, 64, 128
#learning rates: 3e-4, 1e-4, 2e-5, 5e-5, 3e-5
LEARNING_RATE = 1e-4
NUM_EPOCH = 4
BATCH_SIZE = 32

list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_eval=X_val["review_normalise"],
    y_eval=y_val,
    learning_rate=LEARNING_RATE,
    num_epoch=NUM_EPOCH,
    batch_size=BATCH_SIZE
)

plot_loss(list_loss_train, list_loss_eval)

plot_accuracy(list_accuracy_train, list_accuracy_eval)

# Validacao
pred_val, pred_probs = model_bert.predict(X_val["review_normalise"], BATCH_SIZE)
print(classification_report(y_val, pred_val))

plot_matrix_confusion(y_val, pred_val)

"""#### Escolha Hiperparametros melhor Arquitetura"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = [0.1, 0.25, 0.25, 0.3, 0.1] # Pesos para classe a fim de balancear a rede no treinamento
HIDDEN_LAYER_CLASSIFICATION = 1500 # quantidades de neuronios na camada oculta antes da saida, caso 0 so vai ter embedding + saida de classificacao
NUM_EPOCH = 4

search = {
    "learning_rate": [1e-4, 2e-5, 5e-5],
    "batch_size": [8, 32, 128]
}
best_f1_score, best_parameter = get_best_hyperparameters(
    num_scores=NUM_SCORES,
    dropout=DROPOUT,
    max_token=MAX_TOKEN,
    hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
    weight=WEIGHT,
    num_epoch=NUM_EPOCH,
    dict_hiperparameters=search,
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_val=X_val["review_normalise"],
    y_val=y_val
)
best_f1_score, best_parameter

"""### Teste"""

MAX_TOKEN = 165 # definido pela analise de histograma realizada anteriormente
DROPOUT = 0.3
NUM_SCORES = max(y_train)
WEIGHT = [0.1, 0.25, 0.25, 0.3, 0.1]
HIDDEN_LAYER_CLASSIFICATION = 1500

model_bert = Bert(
     num_scores=NUM_SCORES,
     dropout=DROPOUT,
     max_token=MAX_TOKEN,
     hidden_layer_classification=HIDDEN_LAYER_CLASSIFICATION,
     weight=WEIGHT
)

print(model_bert)

summary(model_bert,input_size=(768,),depth=1,batch_dim=1, dtypes=['torch.IntTensor'])

# Train e Val
# recomendacao paper bert
# Batch size: 16, 32
# Learning rate (Adam): 5e-5, 3e-5, 2e-5
# Number of epochs: 2, 3, 4
# recomendacao author bert fine-tuning
#batch sizes: 8, 16, 32, 64, 128
#learning rates: 3e-4, 1e-4, 2e-5, 5e-5, 3e-5
LEARNING_RATE = 1e-4
NUM_EPOCH = 8
BATCH_SIZE = 8

list_accuracy_train, list_accuracy_eval, list_loss_train, list_loss_eval = model_bert.fit(
    X_train=X_train["review_normalise"],
    y_train=y_train,
    X_eval=X_val["review_normalise"],
    y_eval=y_val,
    learning_rate=LEARNING_RATE,
    num_epoch=NUM_EPOCH,
    batch_size=BATCH_SIZE
)

plot_loss(list_loss_train, list_loss_eval)

plot_accuracy(list_accuracy_train, list_accuracy_eval)

# Teste
pred_test, pred_probs = model_bert.predict(X_test["review_normalise"], BATCH_SIZE)
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("BERTimbau Base", accuracy, f1, df_results)
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

"""## LLM (GPT 4 mini)

### Gerar exemplos de treinamento para prompt
"""

def generate_examples_prompt(X, y, score, num_samples, random_state):
    score = y[lambda x: x == score].sample(num_samples, random_state=random_state)
    reviews = X.iloc[score.index]
    return "\tExemplo:\n\t" + "\n\n\tExemplo:\n\t".join(reviews.to_list())

dict_review = dict()
for score in range(1,6):
    reviews = generate_examples_prompt(
        X=X_train["review_normalise"],
        y=y_train,
        score=1,
        num_samples=3,
        random_state=RANDOM_STATE
    )
    dict_review[score] = reviews

"""### Modelo"""

class LLM:

    def __init__(self, client, model_engine):
        self.client = client
        self.model_engine = model_engine
        self.prompt = ""

    def _get_prompt(self, dict_review_examples):
        prompt_base = """"
        ### Contexto ###
        Você é um analista de avaliações de compradores de produtos de telefonia.
        As avaliações estão em português Brasil, as avaliações podem conter abreviações e caracteres especiais em sua maioria.
        Seu trabalho é gerar um score de 1 a 5 para as avaliações dos usuários.
        Escolha somente o score 1,2,3,4 ou 5.
        A resposta deve ser somente um número inteiro de 1 a 5.

        ### Exemplos de avaliações ###

        Score 1:
        {review1}

        Score 2:
        {review2}

        Score 3:
        {review3}

        Score 4:
        {review4}

        Score 5:
        {review5}

        """
        prompt = prompt_base.format(
            review1=dict_review_examples[1],
            review2=dict_review_examples[2],
            review3=dict_review_examples[3],
            review4=dict_review_examples[4],
            review5=dict_review_examples[5]
        )
        return prompt

    def train(self, dict_review_examples):
        self.prompt = self._get_prompt(dict_review_examples)
        print("Prompt gerado com sucesso")

    def _get_llm_review(self, review):
        response = self.client.chat.completions.create(
            model=model_engine,
            messages=[
                {
                "role": "system",
                "content": self.prompt
                },
                {
                "role": "user",
                "content": review
                }
            ],
            temperature=0,
            max_tokens=1
        )
        try:
            return int(response.choices[0].message.content)
        except:
            return None

    def predict(self, X):
        list_predict = list()
        for review in tqdm(X):
            pred = llm._get_llm_review(review)
            list_predict.append(pred)
        return list_predict

"""### Teste"""

# OpenAI API Key variavel ambiente colab
api_key = env_colab.get('OPENAI_API_KEY')

# Model
model_engine = "gpt-4o-mini"

client = OpenAI(api_key=api_key)
llm = LLM(client, model_engine)
llm.train(dict_review)

# Teste

pred_test = llm.predict(X_test["review_normalise"])
accuracy = accuracy_score(y_test, pred_test)
f1 = f1_score(y_test, pred_test, average='macro')
df_results = add_metrics_df("LLM GPT-4 mini", accuracy, f1, df_results)
print("\n\n")
print(classification_report(y_test, pred_test))

plot_matrix_confusion(y_test, pred_test)

"""# Resultados"""

df_results

"""Podemos observar que apesar do BERTimbau (com penalização maior de pesos para correção do desbalanceamento da base) não ter a melhor acurácia, ele possui o melhor f1-score, precision e recall em relação aos demais modelos. Sendo assim, ao considerarmos a combinação de métricas o melhor modelo foi o BERTimbau (BERT pré-treinado em português) seguido do GPT4-mini.

Podemos observar que a base possui um grande desbalanceamento de classes (scores), com o Score 5 correspondendo a 44% da base e o score 1 correspondendo a 22% da base. Ao analisarmos as métricas, notamos que os modelos mais rasos/simples conseguem distinguir bem classes bem separadas como o score 1 e 5, mas não consegue bons resultados nos demais scores devido ao desbalanceamento das classes e a destinção não linear pouco perceptível entre elas.

Os modelos de deep learning Bert e LLM GPT-4 mini se sairam melhores que os demais modelos, ao distinguir melhor os scores 2,3 e 4, conseguindo resultados melhores de f1 score que os demais modelos, tendo assim uma melhor performance ao lidar com dados desbalanceados e com distinção não linear pouco perceptível entre os scores.


Apesar dos resultados alcançados com a utilização de pré-processamento e normalização de abreviações utilizando o ENELVO, podemos perceber que os modelos ainda possuem um desempenho regular abaixo de 60%, esse resultado acontece devido as características da base de dados que possui dados desbalanceados, na língua portuguesa (no qual possui uma complexidade maior), com erros de ortografia e abreviações.
Para trabalhos futuros com a intenção de obter melhores resultados, pretende-se coletar mais dados e melhorar o tratamento/ pré-processamento dos dados utilizando outras técnicas.
"""